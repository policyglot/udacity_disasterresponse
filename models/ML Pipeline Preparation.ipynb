{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# global for Random State \n",
    "seed = 2020\n",
    "\n",
    "# Specific Machine Learning Algorithms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#For Word Processing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', \n",
    "               'averaged_perceptron_tagger', 'stopwords'])\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "#For the Machine Learning Model\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# For Model Fit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# To Save the Model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "\n",
    "database_filepath = \"../data/messages.db\"\n",
    "engine = create_engine('sqlite:///' + database_filepath)\n",
    "#table_name = os.path.basename(database_filepath).replace(\".db\",\"\") + \"_table\"\n",
    "#df = pd.read_sql_table(table_name,engine)\n",
    "df = pd.read_sql(\"SELECT * FROM Messages\", engine)\n",
    "\n",
    "X = df['message']\n",
    "y = df.iloc[:, -36:]\n",
    "# To fix potential multi-output error for 'related' column\n",
    "y['related'].replace(to_replace=2,value=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^A-Za-x0-9]', ' ', text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    final_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline =  Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer(smooth_idf=False)),\n",
    "    ('clf', RandomForestClassifier(random_state=seed))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2020)\n",
    "start = time.time()\n",
    "pipeline.fit(X_train, y_train)\n",
    "# predict on test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = y_test.columns\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=issues)\n",
    "for issue in issues:\n",
    "    print(classification_report(y_test[issue], \n",
    "                            y_pred_df[issue]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {#'scaler__with_mean':['True','False'],\n",
    "             'clf__n_estimators': [100, 300],\n",
    "             'clf__max_features':[5, 8]}\n",
    "\n",
    "cv = GridSearchCV(pipeline, parameters,n_jobs=4, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "cv.fit(X_train, y_train)\n",
    "# predict on test data\n",
    "y_pred = cv.predict(X_test)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "issues = y_test.columns\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=issues)\n",
    "for issue in issues:\n",
    "    print(classification_report(y_test[issue], \n",
    "                            y_pred_df[issue]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n',classification_report(y_train.values, y_prediction_train, target_names=y.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(model,open(model_filepath,'wb'))\n",
    "#classifier = pickle.dumps(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classifier.pkl', 'wb') as classifier:\n",
    "    pickle.dumps(cv, classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
